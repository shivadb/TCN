{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import onnx\n",
    "import onnxruntime as rt\n",
    "from onnx import helper, shape_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 1.9.0\n",
      "onnx version: 1.9.0\n",
      "onnxruntime version: 1.7.2\n"
     ]
    }
   ],
   "source": [
    "print(f'torch version: {torch.__version__}')\n",
    "print(f'onnx version: {onnx.__version__}')\n",
    "print(f'onnxruntime version: {rt.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorCache(nn.Module):\n",
    "    def __init__(self, tensor):\n",
    "        super(TensorCache, self).__init__()\n",
    "        self.register_buffer('cache', tensor)\n",
    "#         self.cache = tensor\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # assert x.size() == self.cache[:,:,0:1].size()\n",
    "        cache_update = torch.cat((self.cache[:,:,1:], x.detach()), dim=2)\n",
    "        self.cache[:,:,:] = cache_update\n",
    "        return self.cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "outputs_[i]->uses().empty()INTERNAL ASSERT FAILED at \"/opt/conda/conda-bld/pytorch_1623448238472/work/torch/csrc/jit/ir/ir.cpp\":1226, please report a bug to PyTorch. ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-d339466faefe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensorCache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtc_script\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscript\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtc_trace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/torch19/lib/python3.9/site-packages/torch/jit/_trace.py\u001b[0m in \u001b[0;36mtrace\u001b[0;34m(func, example_inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit)\u001b[0m\n\u001b[1;32m    733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 735\u001b[0;31m         return trace_module(\n\u001b[0m\u001b[1;32m    736\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m             \u001b[0;34m{\u001b[0m\u001b[0;34m\"forward\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mexample_inputs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/torch19/lib/python3.9/site-packages/torch/jit/_trace.py\u001b[0m in \u001b[0;36mtrace_module\u001b[0;34m(mod, inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0mexample_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             module._c._create_method_from_trace(\n\u001b[0m\u001b[1;32m    953\u001b[0m                 \u001b[0mmethod_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: outputs_[i]->uses().empty()INTERNAL ASSERT FAILED at \"/opt/conda/conda-bld/pytorch_1623448238472/work/torch/csrc/jit/ir/ir.cpp\":1226, please report a bug to PyTorch. "
     ]
    }
   ],
   "source": [
    "tc = TensorCache(torch.zeros(1,1,10))\n",
    "tc_script = torch.jit.script(tc)\n",
    "tc_trace = torch.jit.trace(tc, torch.tensor([[[0.0]]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]]])\n",
      "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 1., 2.]]])\n",
      "tensor([[[0., 0., 0., 0., 0., 0., 0., 1., 2., 3.]]])\n",
      "tensor([[[0., 0., 0., 0., 0., 0., 1., 2., 3., 4.]]])\n",
      "\n",
      "Convert using script:\n",
      "tensor([[[0., 0., 0., 0., 0., 1., 2., 3., 4., 1.]]])\n",
      "tensor([[[0., 0., 0., 0., 1., 2., 3., 4., 1., 2.]]])\n",
      "tensor([[[0., 0., 0., 1., 2., 3., 4., 1., 2., 3.]]])\n",
      "tensor([[[0., 0., 1., 2., 3., 4., 1., 2., 3., 4.]]])\n",
      "\n",
      "Convert using trace:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tc_trace' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-c902febc5c12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Convert using trace:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2.\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3.\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtc_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tc_trace' is not defined"
     ]
    }
   ],
   "source": [
    "print('Original:')\n",
    "for inp in [1.,2.,3.,4.]:\n",
    "    print(tc(torch.tensor([[[inp]]])))\n",
    "\n",
    "print()\n",
    "print('Convert using script:')\n",
    "for inp in [1.,2.,3.,4.]:\n",
    "    print(tc_script(torch.tensor([[[inp]]])))\n",
    "    \n",
    "print('')\n",
    "print('Convert using trace:')\n",
    "for inp in [1.,2.,3.,4.]:\n",
    "    print(tc_trace(torch.tensor([[[inp]]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_out = tc(torch.tensor([[[0.5]]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph(%input : Float(1, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu),\n",
      "      %cache : Float(1, 1, 10, strides=[10, 10, 1], requires_grad=0, device=cpu),\n",
      "      %74 : Float(1, 1, 9, strides=[10, 10, 1], requires_grad=0, device=cpu),\n",
      "      %75 : Long(3, strides=[1], requires_grad=0, device=cpu),\n",
      "      %87 : Long(10, strides=[1], requires_grad=0, device=cpu),\n",
      "      %88 : Long(1, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu),\n",
      "      %89 : Long(1, 1, strides=[1, 1], requires_grad=0, device=cpu),\n",
      "      %92 : Long(3, strides=[1], requires_grad=0, device=cpu),\n",
      "      %93 : Long(1, strides=[1], requires_grad=0, device=cpu),\n",
      "      %94 : Long(1, strides=[1], requires_grad=0, device=cpu),\n",
      "      %95 : Long(1, strides=[1], requires_grad=0, device=cpu),\n",
      "      %98 : Long(3, strides=[1], requires_grad=0, device=cpu)):\n",
      "  %7 : Float(1, 1, 10, strides=[10, 10, 1], device=cpu) = onnx::Concat[axis=2](%74, %input) # <ipython-input-4-95994830632f>:9:23\n",
      "  %9 : Float(1, 1, 10, device=cpu) = onnx::Expand(%7, %75) # <ipython-input-4-95994830632f>:10:8\n",
      "  %39 : Long(3, device=cpu) = onnx::ConstantOfShape[value={1}](%93)\n",
      "  %40 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}]()\n",
      "  %41 : Long(3, strides=[1], device=cpu) = onnx::Mul(%39, %40)\n",
      "  %42 : Bool(3, strides=[1], device=cpu) = onnx::Equal(%92, %41)\n",
      "  %43 : Long(3, strides=[1], device=cpu) = onnx::Where(%42, %39, %92)\n",
      "  %44 : Long(1, 1, 10, device=cpu) = onnx::Expand(%88, %43)\n",
      "  %45 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={-1}]()\n",
      "  %46 : Long(1, 1, 10, 1, strides=[10, 10, 1, 1], device=cpu) = onnx::Unsqueeze(%44, %45)\n",
      "  %48 : Long(3, device=cpu) = onnx::ConstantOfShape[value={1}](%94)\n",
      "  %49 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}]()\n",
      "  %50 : Long(3, strides=[1], device=cpu) = onnx::Mul(%48, %49)\n",
      "  %51 : Bool(3, strides=[1], device=cpu) = onnx::Equal(%92, %50)\n",
      "  %52 : Long(3, strides=[1], device=cpu) = onnx::Where(%51, %48, %92)\n",
      "  %53 : Long(1, 1, 10, device=cpu) = onnx::Expand(%89, %52)\n",
      "  %54 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={-1}]()\n",
      "  %55 : Long(1, 1, 10, 1, strides=[10, 10, 1, 1], device=cpu) = onnx::Unsqueeze(%53, %54)\n",
      "  %57 : Long(3, device=cpu) = onnx::ConstantOfShape[value={1}](%95)\n",
      "  %58 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}]()\n",
      "  %59 : Long(3, strides=[1], device=cpu) = onnx::Mul(%57, %58)\n",
      "  %60 : Bool(3, strides=[1], device=cpu) = onnx::Equal(%92, %59)\n",
      "  %61 : Long(3, strides=[1], device=cpu) = onnx::Where(%60, %57, %92)\n",
      "  %62 : Long(1, 1, 10, device=cpu) = onnx::Expand(%87, %61)\n",
      "  %63 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={-1}]()\n",
      "  %64 : Long(1, 1, 10, 1, strides=[10, 10, 1, 1], device=cpu) = onnx::Unsqueeze(%62, %63)\n",
      "  %65 : Long(1, 1, 10, 3, strides=[30, 30, 3, 1], device=cpu) = onnx::Concat[axis=-1](%46, %55, %64)\n",
      "  %72 : Float(1, 1, 10, device=cpu) = onnx::Reshape(%9, %98)\n",
      "  %output : Float(1, 1, 10, strides=[10, 10, 1], requires_grad=0, device=cpu) = onnx::ScatterND(%cache, %65, %72) # <ipython-input-4-95994830632f>:10:8\n",
      "  return (%output)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.onnx.export(\n",
    "    tc_script,\n",
    "    torch.tensor([[[0.0]]]),\n",
    "    f'onnxrt_test.onnx',\n",
    "    export_params=True,\n",
    "    do_constant_folding=True,\n",
    "    keep_initializers_as_inputs=True,\n",
    "    opset_version=13,\n",
    "    input_names = ['input'],\n",
    "    output_names = ['output'],\n",
    "    example_outputs=ex_out,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model = onnx.load('onnxrt_test.onnx')\n",
    "onnx.checker.check_model(onnx_model)\n",
    "inferred_model = shape_inference.infer_shapes(onnx_model)\n",
    "onnx.checker.check_model(inferred_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_path = 'onnxrt_test.onnx'\n",
    "ort_session = rt.InferenceSession(str(onnx_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX Runtime:\n",
      "[[[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]]\n",
      "[[[0. 0. 0. 0. 0. 0. 0. 0. 0. 2.]]]\n",
      "[[[0. 0. 0. 0. 0. 0. 0. 0. 0. 3.]]]\n",
      "[[[0. 0. 0. 0. 0. 0. 0. 0. 0. 4.]]]\n"
     ]
    }
   ],
   "source": [
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "\n",
    "print('ONNX Runtime:')\n",
    "for inp in [1.,2.,3.,4.]:\n",
    "    ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(torch.tensor([[[inp]]]))}\n",
    "    ort_outs = ort_session.run(None, ort_inputs)\n",
    "    print(ort_outs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 1.9.0\n",
      "onnx version: 1.9.0\n",
      "onnxruntime version: 1.7.2\n",
      "Original:\n",
      "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]]])\n",
      "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 1., 2.]]])\n",
      "tensor([[[0., 0., 0., 0., 0., 0., 0., 1., 2., 3.]]])\n",
      "tensor([[[0., 0., 0., 0., 0., 0., 1., 2., 3., 4.]]])\n",
      "\n",
      "Convert using script:\n",
      "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]]])\n",
      "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 1., 2.]]])\n",
      "tensor([[[0., 0., 0., 0., 0., 0., 0., 1., 2., 3.]]])\n",
      "tensor([[[0., 0., 0., 0., 0., 0., 1., 2., 3., 4.]]])\n",
      "\n",
      "graph(%input : Float(1, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu),\n",
      "      %8 : Float(1, 1, 9, strides=[10, 10, 1], requires_grad=0, device=cpu)):\n",
      "  %output : Float(1, 1, 10, strides=[10, 10, 1], requires_grad=0, device=cpu) = onnx::Concat[axis=2](%8, %input)\n",
      "  return (%output)\n",
      "\n",
      "ONNX Runtime:\n",
      "[[[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]]\n",
      "[[[0. 0. 0. 0. 0. 0. 0. 0. 0. 2.]]]\n",
      "[[[0. 0. 0. 0. 0. 0. 0. 0. 0. 3.]]]\n",
      "[[[0. 0. 0. 0. 0. 0. 0. 0. 0. 4.]]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import onnx\n",
    "import onnxruntime as rt\n",
    "from onnx import helper, shape_inference\n",
    "\n",
    "print(f'torch version: {torch.__version__}')\n",
    "print(f'onnx version: {onnx.__version__}')\n",
    "print(f'onnxruntime version: {rt.__version__}')\n",
    "\n",
    "class InputCache(nn.Module):\n",
    "    def __init__(self, tensor):\n",
    "        super(InputCache, self).__init__()\n",
    "        self.register_buffer('cache', tensor)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # self.cache[:,:,:-1] = self.cache.clone()[:,:,1:]\n",
    "        # self.cache[:,:,-1:] = x.detach()\n",
    "        cache_update = torch.cat((self.cache[:,:,1:], x.detach()), dim=2)\n",
    "        self.cache = cache_update\n",
    "        return self.cache\n",
    "\n",
    "tc = InputCache(torch.zeros(1,1,10))\n",
    "tc_script = torch.jit.script(InputCache(torch.zeros(1,1,10)))\n",
    "\n",
    "# Make sure torch code is doing what it should\n",
    "print('Original:')\n",
    "for inp in [1.,2.,3.,4.]:\n",
    "    print(tc(torch.tensor([[[inp]]])))\n",
    "\n",
    "print()\n",
    "print('Convert using script:')\n",
    "for inp in [1.,2.,3.,4.]:\n",
    "    print(tc_script(torch.tensor([[[inp]]])))\n",
    "\n",
    "# Reinitialize\n",
    "tc = InputCache(torch.zeros(1,1,10))\n",
    "tc_script = torch.jit.script(InputCache(torch.zeros(1,1,10)))\n",
    "\n",
    "ex_out = tc(torch.tensor([[[0.5]]]))\n",
    "\n",
    "print()\n",
    "torch.onnx.export(\n",
    "    tc_script,\n",
    "    torch.tensor([[[0.0]]]),\n",
    "    f'onnxrt_test.onnx',\n",
    "    export_params=True,\n",
    "    do_constant_folding=True,\n",
    "    keep_initializers_as_inputs=True,\n",
    "    opset_version=12,\n",
    "    input_names = ['input'],\n",
    "    output_names = ['output'],\n",
    "    example_outputs=ex_out,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Use onnx tools to make sure model is valid\n",
    "onnx_model = onnx.load('onnxrt_test.onnx')\n",
    "onnx.checker.check_model(onnx_model)\n",
    "inferred_model = shape_inference.infer_shapes(onnx_model)\n",
    "onnx.checker.check_model(inferred_model)\n",
    "\n",
    "# Run inference session\n",
    "onnx_path = 'onnxrt_test.onnx'\n",
    "ort_session = rt.InferenceSession(str(onnx_path))\n",
    "\n",
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "\n",
    "print('ONNX Runtime:')\n",
    "for inp in [1.,2.,3.,4.]:\n",
    "    ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(torch.tensor([[[inp]]]))}\n",
    "    ort_outs = ort_session.run(None, ort_inputs)\n",
    "    print(ort_outs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch19",
   "language": "python",
   "name": "torch19"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
